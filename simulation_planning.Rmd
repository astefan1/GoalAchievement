---
title: "A Comparison of Goal Achievement Methods"
output:
  html_document:
    df_print: paged
---
```{r echo=FALSE}
load("/Users/angelika/Documents/Projects/Goal Achievement/GoalAchievement/fitB.RData")
```

### Experimental Design: 
Participants are measured before and 1 month after an intervention directed at achieving their fitness goals. There are three treatment groups and one control group. Treatment groups are instructed to follow one out of three goal achievement methods: Implementation Intentions, Mental Contrasting, or Mental Contrasting with Implementation Intentions.

Participants' physical activity is measured on the __Health Contribution Index__, derived from the Godin scale, at both timepoints.

Participants recruited in the study are insufficiently active according to the American College of Sports Medicine. We are planning to recruit 250 participants.

### Research Question & Hypotheses
We are interested in comparing the treatment effects of the isolated goal achievement methods (Implementation Intentions, Mental Contrasting) to the combined goal achievement method (Mental Contrasting with Implementation Intentions). Our main hypothesis predicts that:

* All three interventions improve physical activity compared to the control group
* The two isolated methods do not differ in their efficacy
* The combined method is more effective than the two isolated methods

### Visualization of expected outcomes

```{r echo=FALSE}
# ====================== DATA SIMULATION =======================================
N <- 250                    # overall sample size
meanControl <- 13.4         # post treatment control group meant
sd_measurement <- sqrt(10)  # measurement error
sd_trueScore <- sqrt(42)    # true variability in pre-treatment scores
effSingle <- 2.5            # effect of single treatments
effCombi <- 5               # effect of combined treatment
betaPhysActPre <- 1         # influence of pre-treatment on post-treatment scores

# Dummy-coded treatment variables
treatment <- rep(c(0:3), length.out=N)
control <- as.numeric(treatment==0)
impInt <- as.numeric(treatment==1)
mentCont <- as.numeric(treatment==2)
combiTreat <- as.numeric(treatment==3)

# Physical Activity pre treatment (same for all groups, mean-centered)
physAct_PreTrue <- rnorm(N, mean=0, sd=sd_trueScore)
physAct_Pre <- physAct_PreTrue+rnorm(N, mean=0, sd=sd_measurement)

# Physical Activity post treatment
physAct_Post <- meanControl + betaPhysActPre*physAct_PreTrue + effSingle*impInt + effSingle*mentCont + effCombi*combiTreat + rnorm(N, 0, sd_measurement)

# Combine to dataframe
treatment <- as.factor(treatment)
levels(treatment) <- c("Control", "Imp. Intentions", "M. Contrasting", "Combined")
id <- 1:N
dat <- data.frame(id, physAct_Post, physAct_Pre = physAct_Pre+meanControl, treatment)

```

```{r echo=FALSE}
# =================== PLOT SIMULATED DATA ======================================

library(ggplot2)
library(reshape2)

dat_long <- reshape2::melt(dat, id.vars=c("id", "treatment"), value.name = "physAct")
ggplot(dat_long, aes(y=physAct, x = treatment, colour = variable)) +
  geom_boxplot(outlier.shape = 8) +
  geom_jitter(aes(color=variable), alpha=0.3, position=position_jitterdodge()) +
  theme_classic() +
  ylab("Physical Activity") +
  xlab("Treatment Group") +
  scale_colour_discrete(name="Timepoint", labels = c("Post", "Pre"))
```

Based on previous research, the pre-treatment scores in the visualization above are assumed to follow a normal distribution with mean 13.4 and standard deviation 7.2. Post-treatment scores are assumed to be 2.5 points higher on average for the isolated methods and 5 points higher on average for the combined method. This is equivalent to 50\% and 100\% of participants adding one moderate physical activity per week, respectively.

The associated data structure looks as follows:

```{r echo=FALSE}
head(dat)
```


### Statistical Model
We compute a Bayesian linear regression, predicting post-treatment scores from treatment group and controlling for pre-treatment scores (ANCOVA model). Treatment is dummy-coded with the control group as a basis. The pre-treatment scores are mean-centered.

$$
Post_{i} \sim N(\beta_0+\beta_1Pre_{i}+\beta_2impInt_{i}+\beta_3mentCont_{i}+\beta_4Combined_{i}, \sigma)
$$

In the model, $\beta_2$ represents the effect of Implementation Intentions, $\beta_3$ represents the effect of Mental Contrasting, and $\beta_4$ represents the effect of the Combined Treatment. If $\beta_{2,3,4}$ are positive, the treatment groups have higher physical activity scores post-treatment than the control group. $\beta_0$ is the intercept parameter and indicates the mean physical activity score post-treatment in the control group. $\beta_1$ is a regression weight representing the correlation between pre-treatment and post-treatment scores. For example, if $\beta_1 = 1$, a person one point above the mean pre-treatment is predicted to be one point above their respective group mean post-treatment. $\sigma$ denotes the variability of physical activity scores between persons at any level of the predictor variables.

Same as a frequentist ANCOVA model, the statistical model assumes linearity of the pre-treatment effect, absence of treatment-pretest interaction, and the absence of group differences in the pre-treatment scores.

### Prior Distributions

The Bayesian model requires __prior distributions__ on all parameters. We opt for weakly informative prior distributions on the regression coefficients that constrict the model predictions to a reasonable range of values but do not favor any direction of effect. For the intercept parameter, we use an informed prior distribution that centers around the mean score of insufficiently active people in the normative sample, but displays considerable uncertainty. Note that inference about the regression coefficients $\beta_2, \beta_3$, and $\beta_4$ is independent of the intercept parameter. For the standard deviation term, we use the default prior implemented in the R-package `brms`.

Specifically, we use the following prior distributions:

$\beta_0 \sim N(\mu_{\beta_0}=13.4, \sigma_{\beta_0}=15)$

$\beta_{1,...,4} \sim N_+(\mu_{\beta_{1,...,4}}=0, \sigma_{\beta_{1,...,4}}=15)$

$\sigma \sim t_+(\nu_\sigma=3, \mu_\sigma = 0, \sigma_\sigma=7.5)$

```{r echo=FALSE, message=FALSE}
library(brms)
par(mfrow=c(3,1))
curve(dnorm(x, mean=13.4, sd=15), xlim=c(0, 50), xlab="Intercept", ylab="Density", main="Prior Distributions", yaxt="n")
curve(dnorm(x, mean=0, sd=15), xlim=c(-50, 50), xlab="Regression Coefficients", ylab="Density", yaxt="n")
curve(dstudent_t(x, df=3, mu=0, sigma=7.5), xlim=c(0, 50), xlab="Standard Deviation", ylab="Density", yaxt="n")
```

__Prior predictive checks__ demonstrate that the chosen prior distributions allow for a wide range of plausible values. The plot shows that most predicted values range in the interval of [3; 30], with slightly less variation for the control group than for the experimental groups due to the added uncertainty about treatment effects.

```{r results="hide", message=FALSE, echo=FALSE}
modelpriors <- c(set_prior("normal(0, 15)", class = "b"),
                 set_prior("normal(13.4, 15)", class = "Intercept", lb=0),
                 set_prior("student_t(3, 0, 7.5)", class = "sigma"))

# Draw from priors #

dat$physAct_Pre_centered <- dat$physAct_Pre-mean(dat$physAct_Pre)

fitB <- update(fitB,
               physAct_Post ~ physAct_Pre_centered + treatment,
               newdata = dat, 
               family = "gaussian", 
               prior = modelpriors,
               sample_prior = "only",
               verbose = FALSE)
            
prior_summary(fitB)

newdata <- expand.grid(treatment = c("Control", "Imp. Intentions", "M. Contrasting", "Combined"),
                       physAct_Pre_centered = 0)

priorPred <- posterior_predict(fitB,
                               newdata = newdata)
```

```{r echo=FALSE, warning=FALSE}

priorPred_long <- reshape2::melt(priorPred)
priorPred_long$Var2 <- as.factor(priorPred_long$Var2)

# For average pre-treatment score
ggplot(priorPred_long, aes(x = Var2, y = value)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(alpha=0.03) +
  theme_classic() +
  geom_hline(yintercept = c(-25, -50, 0, 25, 50), colour = "grey", linetype = "dashed") +
  xlab("Treatment Group") +
  ylab("Physical Activity (Godin Score)") +
  scale_x_discrete(labels=c("Control", "Imp. Intentions", "M. Contrasting", "Combined")) +
  ylim(-100,100)
```

### Planned Analyses (Main Hypothesis)

```{r results="hide", message=FALSE, echo=FALSE}
fitB <- update(fitB,
               physAct_Post ~ physAct_Pre_centered + treatment,
               newdata = dat, 
               family = "gaussian", 
               prior = modelpriors,
               sample_prior = "yes",
               verbose = FALSE)
```


#### Convergence Diagnostics

After fitting the model using `brms`, we will evaluate several convergence diagnostics to ensure that the posterior samples stem from the joint posterior distribution.

As a first step, we visually inspect trace plots to check for autocorrelation and proper mixing of the chains. Chains should look like "hairy caterpillars" if autocorrelation is acceptable.

```{r echo=FALSE}
library(MCMCvis)
MCMCtrace(fitB, type = "trace", ind = TRUE, pdf = FALSE,
          params = c("Intercept", 
                     "physAct_Pre_centered", 
                     "treatmentImp.Intentions", 
                     "treatmentM.Contrasting", 
                     "treatmentCombined", 
                     "sigma"))

```

In a second step, we inspect the R-hat statistic, as well as bulk and tail effective sample sizes. We will deem R-hat values smaller than 1.01, as well as a effective sample sizes larger than 400 as sufficient.

```{r echo=FALSE}
summary(fitB)
```

If convergence diagnostics fail, we will increase the number of iterations per chain and/or use thinning to improve estimation. 

#### Inference

In the context of the statistical model, the main hypothesis will be tested using the following analyses:

__All three interventions improve physical activity compared to the control group__

For each treatment effect ($\beta_2, \beta_3, \beta_4$), will investigate the Bayes factor comparing the predictive adequacy of a model where the respective treatment effect is zero and a model where the treatment effect is allowed to differ from zero, as well as the posterior probabilities of $\beta_2$, $\beta_3$, and $\beta_4$ being larger than zero.

```{r}
hypothesis(fitB, "treatmentImp.Intentions = 0")
hypothesis(fitB, "treatmentM.Contrasting = 0")
hypothesis(fitB, "treatmentCombined = 0")
postSamples <- as_draws_df(fitB)
sum(postSamples$b_treatmentImp.Intentions > 0)/nrow(postSamples)
sum(postSamples$b_treatmentM.Contrasting > 0)/nrow(postSamples)
sum(postSamples$b_treatmentCombined > 0)/nrow(postSamples)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidybayes)

postSamples_long <- melt(postSamples[, c("b_treatmentImp.Intentions", "b_treatmentM.Contrasting", "b_treatmentCombined")])

ggplot(postSamples_long, aes(x=value, y=variable, fill=stat(x>0))) +
  stat_halfeye() +
  theme_classic() +
  scale_fill_manual(values=c("gray80", "skyblue")) 
```

__The two isolated methods do not differ in their efficacy__

We use a Bayes factor to test this hypothesis. Specifically, we will use the Savage-Dickey density ratio to compare the predictive accuracy of two models: One where the parameters $\beta_3$ and $\beta_2$ are equal (i.e., their difference is zero), and one where the parameters are free to vary.

```{r}
hypothesis(fitB, "treatmentImp.Intentions - treatmentM.Contrasting = 0")
```

The Bayes factor in favor of H0 ($\text{BF}_{01}$) is displayed in the "Evid. Ratio" column above. Values larger than one indicate that the data are more likely to have occurred under the model that assumes equality.

__The combined method is more effective than the two isolated methods__

To assess this claim, we will

(a) test whether the data are more likely to have occurred under a model that restricts $\beta_4$ to be the same as $\beta_3$ or $\beta_2$, respectively; and

(b) compute the posterior probability that the difference between $\beta_4$ and $\beta_3$ or $\beta_2$ is larger than zero, respectively.

```{r}
sum(postSamples$b_treatmentCombined-postSamples$b_treatmentImp.Intentions > 0)/nrow(postSamples)
sum(postSamples$b_treatmentCombined-postSamples$b_treatmentM.Contrasting > 0)/nrow(postSamples)

hypothesis(fitB, "treatmentCombined - treatmentImp.Intentions = 0")
hypothesis(fitB, "treatmentCombined - treatmentM.Contrasting = 0")
```







